---
title: "Investigating IPR due to N. parisii and N. ausubeli infection"
output:
  html_document:
    toc: true
    toc_depth: 1
    df_print: paged
---
#Introduction  
We will be performing a differential expression analysis on data gathered by Emily during her time at the Broad Institute. To do so we will be using R and R "packages". Specifically, edgeR and limma, which will perform the heavy-lifting. We will also use Glimma later to make some interactive graphs. 
  
##Packages
A package is a bundle of code that generally extends the capabilities of R (or other language). In a generalized example, a user has a problem that takes several steps to complete and must be solved repeatedly. In response our user may write a package that contains a single function which performs all these steps. Now our user can install this package and solve the problem in a single step. 
  
##Reading this document  
This document is an R Notebook (an alternative to an R script) and was written in Rstudio using R markdown language. When reading this document, users will see a few different formats. First, there will be plain text like you're reading now. Second, are code blocks which will display the actual R code used to perform the calculations. They are generally gray or otherwise colored in and surrounded by a border. 

```{r}
#This is a code block
```

There may also be `some code` identified in the plain text. I will always name `variable` and `functions()` using this markup, and note the () after functions means it's a function.  

Third will be the output which will almost always follow a code block. Sometimes they may show `##` before each line:  
```{r}
#Code block followed by ##output
dim(cars)
```

While other times they may be an interactive piece of output:  
```{r}
#Click 'Next' to see all cars listed
head(cars, n=15)
```

or a graph:
```{r}
plot(cars$speed)
```

One important concept to understand is commenting. Commenting uses a `#` (in R and other languages, but not all) to tell the computer that the line following the `#` is meant for humans and should not be interpreted as code. See some of the previous code blocks to see this in action. 

#Setup
##Install packages
The first step is to install packages. In R there are a few ways of doing this. Most often the `install.packages()` will be used to fetch a package from the [CRAN repository](https://cran.r-project.org/). For our use, we use a different repository, [Bioconductor](https://bioconductor.org/), which has its own installer function, `biocLite()`. The first step here is to direct R to look at the bioconductor repository using `source()`, and then to use `biocLite()` to install the necessary packages. 

```{r eval=F}
source("http://bioconductor.org/biocLite.R")
biocLite()
biocLite("edgeR")
biocLite("limma")
biocLite("Glimma")
```
Note: the installation step only needs to be completed once. After a package has been installed on your computer, you do not need to reinstall it. Although, you may need to update a package if a new version is released. 

##Load packages (libraries)  
The second step after package installation is to load the libraries into your R session. Installing the packages places the required files onto your computer, but you need to tell R that you want to access those files. This is important because as time goes on you will install more and more packages, and this could cause issues if they were all simultaneously loaded. It is good practice to only load the libraries you need. Loading libraries is something that must be performed everytime your R session is closed.
```{r}
library(edgeR)
library(limma)
```

In this particular instance, it looks like loading edgeR automatically loads the limma package. This is not always the case, and I am showing both libraries in order to be explicit. It is also worth knowing that loading required packages does not necessarily need to happen here. Indeed, futher along in this analysis we will load more libraries in an on-demand manner.  

#Import data
After loading necessary libraries, the next step is to load the relevant data into our R environment. There are a variety of ways to do this, and this will be just one example. I use the `read.csv()` function here, but `read.table()` or any number of other functions could be used to import the data. I then use `dim()` to check the dimensions of this matrix (row, column), and `head()` to inspect the first 6 rows of the matrix.  

Note: In this case we have a single matrix file, but this is not always true. In many instances each sample will have an individual file with the raw count data. In those cases, a user has several options, including loading all files as seperate variables (i.e., `genes_rsem_1` and `genes_rsem_2`), or concatenating individual files into a single variable. 
```{r}
##=====================##

genes_rsem <- read.csv("GSE106647_gene_counts.txt", stringsAsFactors = F, sep="\t")
dim(genes_rsem)
colnames(genes_rsem)
head(genes_rsem)
```

Here we can see that we have successfully loaded the data. We can see how large it is using `dim()`, `colnames()` gives us the column names which should be the sample names, and `head()` lets us check the first few rows of the counts matrix. In total, all of these small sanity checks suggest that we have a complete data set and the data looks to be in the proper format.

##Set up counts matrix
In this particular analysis, this step is somewhat superfluous. We could perform the next step using our `genes_rsem` object. However, I am leaving this step in for continuity with other analyses I've performed in this laboratory, and because there are certain situations where it would be useful to have both a `genes_rsem` object and a `counts_matrix` object, even if they are nearly identical. 
```{r}
##=====================##

counts_matrix <- genes_rsem[,2:25]
rownames(counts_matrix) <- genes_rsem[,1]
head(counts_matrix)
```

##Generate DGEList object
There are a number of ways to complete the previous two steps, and it's not critical how they are accopmlished. The end product of these import data steps is to have all the necessary data loaded into R in order to generate a DGEList object. A DGEList object is a class of data storage object that edgeR and limma functions can read and operate on, and additional information about them can be found by reading the edgeR documentation.  
```{r}
x <- DGEList(counts=counts_matrix, genes = row.names(counts_matrix))
class(x)
x
```

NOTE: There is a second function, `readDGE()`, that will also generate our DGElist object. `readDGE()` may be used to concatenate several sample files into a single DGElist. Since our data was provided in a single matrix file, we used the `DGEList()` function. 

#Organize data
Now that we have imported our data and created our DGEList object we move to the next step. Organizing the data is a crucial step for almost any analysis. We are going to revise the names, assign experimental groups (e.g., infected or uninfected, wt or mutant, etc.). Many of these steps are not set in stone and must be adapted for each new experiment.  

##Simplify names
This step is not necessary, but it is a quality of life improvement that will make things easier moving forward. Earlier, when we used `colnames()` we could see the extraneous information in the sample names, most notably the "Counts_" at the start of each name. I use the `gsub()` function to remove this "Counts_"tag that we are not interested in. We save this to the `samplenames` object so we can check that our `gsub()` worked as expected. This is not strictly neccessary though, and one could save directly to `colnames(x)`. If you're wrong though, you'll need to reload the initial steps to reset `x`.  
```{r}
##=====================##

samplenames <- gsub("Counts_","",colnames(x))
samplenames
colnames(x) <- samplenames
colnames(x)
```

##Group by treatments
We sort our experiment into groups. In this case our groups are the different genotypes sequenced. Other factors that could arise: date harvested, date sequenced, treated with drug, and many others. In general, one should be aware of "batch" effects, that is, some factor that contributes to variabilty other than our experimental factors. Those sources of variability could be any of the factors listed already, and any number of other items. Further down the workflow, we will see how to investigate some of these potential batch effects.  

For our analysis, we only have one variable, which is treatment with ERTmx or untreated. The following steps assign the treatment group to the samples in the `$group` slot.

```{r}
##=====================##

group <- as.factor(rep(c("N2","adr","adr_rrf3","adr_rrf3_rde1","adr_rrf3_rde4","rrf3"),c(4,4,4,4,4,4)))
x$samples$group <- group
x$samples
```

Looking at the above chart, we can see our 9 samples, and that each have been grouped into the proper treatment category. lib.size refers to the library size and is the sum of every value in the column. The norm.factors, are normalization factors which we have not calculated yet and are thus set to their default value of 1.  

Depending on the experiment, there may be more to do at this step (probably not less), but our analysis is relatively straight forward.

##Data pre-processing
Before we begin to analyze the data, there are certain steps to take to prepare the data.  

This first block is taking the counts-per-million (cpm) of the reads aswell as the logcpm. These values will be used further down to perform and highlight some of the pre-processing we will perform.  
```{r}
cpm <- cpm(x)
lcpm <- cpm(x, log=T)
head(cpm)
```

##Remove lowly expressed genes
Here, we use the cpm values calculated previous to remove lowly expressed genes. We'll begin by looking at the size of our data object. In the case of a DGEList object, using the `dim()` function simply shows us how many rows/genes we have, and we'll perform the same test after we throw out lowely expressed genes. The functions we will use in this analysis are more robust when genes that are essentially zero are removed.  

1. First expression checks the number of genes that have zero counts in every sample.  

2. Second expression creates a vector of genes that have cpm >1 in at least three rows. We choose three because that is the number of replicates we have, however, this is an arbitrary number and the cutoff is flexible.  

3. The last expression keeps the genes that match the cutoff described above and refactors the library sizes now that we have dropped a number of genes along with their count data.


```{r}
dim(x)
#1. Check the number of genes that are zero in each sample
table(rowSums(x$counts==0)==24)
#2. Check and store the genes that meet our threshold
keep.exprs <- rowSums(cpm>1)>=4
#3. Keep the genes that meet our threshold, drop genes that do not
x <- x[keep.exprs,, keep.lib.sizes=F]
dim(x)
```

In this case, we start out with 17,499 genes. This is fewer than the total number of genes found in *C. elegans* (>20,000). This suggests that a number of genes have already been filtered out. 

Our next test, the `table(rowSums(x$counts==0)==24)`, shows FALSE 17,499, which means that there were no genes that had no counts across all 24 samples. I would predict that the raw data we downloaded from GEO and loaded here already removed genes with no count data across all samples. 

Our next threshold, `keep.exprs <- rowSums(cpm>1)>=4`, which will keeps genes with at least a cpm of 1 in at least four samples, is then performed. We can see in our final `dim(x)` call, that we reduce our total gene count to 14,767, a reduction of ~3,000 genes. Removing the low count genes will help to make the differential expression analysis more robust.

##Graphs showing filtering of low-count genes
These graphs show how the distribution of our data changes after our thresholding and pre-processing. On the left, we see the large spike of genes at very low counts. On the right, we see we have eliminated these low-count genes. 

```{r}
library(RColorBrewer)
nsamples <- ncol(x)
col <- brewer.pal(nsamples, 'Paired')
par(mfrow=c(1,2))
plot(density(lcpm[,1]), col=col[1], lwd=2, ylim=c(0,0.21), las=2, main="", xlab="")
title(main="Raw data", xlab="log-cpm")
abline(v=0, lty=3)
for (i in 2:nsamples) {
  den <- density(lcpm[,i])
  lines(den$x, den$y, col=col[i], lwd=2)
}
legend("topright", samplenames, text.col = col, bty="n", cex=0.6)
lcpm <- cpm(x, log=T)
plot(density(lcpm[,1]), col=col[1], lwd=2, ylim=c(0,0.21), las=2, main="", xlab="")
title(main="Filtered data", xlab="log-cpm")
abline(v=0, lty=3)
for (i in 2:nsamples) {
  den <- density(lcpm[,i])
  lines(den$x, den$y, col=col[i], lwd=2)
}
legend("topright", samplenames, text.col = col, bty="n", cex=0.6)
```


##Normalizing gene expression distributions
Our data is in raw scale prior to normalization. `calcNormFactors()` function here uses trimmed mean of M-values, "TMM", to normalize the samples based on library size. Picture it this way, we don't want to say that transcript x is more abundant in sample A versus sample B simply because sample A has more reads.
```{r}
x$samples$norm.factors
x <- calcNormFactors(x, method = "TMM")
x$samples$norm.factors
```

#Unsupervised clustering
##Looking for batch effects
Dimension 1 (x-axis in left plot) represents the largest contributor to the variance within the data. In general, one would try to catch batch effects here. If samples were prepared on seperate dates, or ran on seperate lanes, a user could graph the samples here, labelled by their preparation dates, and see if samples cluster by date in a higher dimension. This would suggest a batch effect related to preparation date. Dealing with batch effects (other than ignoring them) is a complicated matter.
```{r}
lcpm <- cpm(x, log=T)
par(mfrow=c(1,2))
col.group <- group
levels(col.group) <- brewer.pal(nlevels(col.group), "Set1")
col.group<-as.character(col.group)
plotMDS(lcpm, labels = group, col=col.group)
title(main="Genotype - dim 1,2")
plotMDS(lcpm, labels = group, col=col.group, dim=c(3,4))
title(main="Genotype - dim 3,4")
```

These data do not cluster very well, particularly the N2 control samples. This doesn't disqualify the data set, but users should keep in mind that there was a lot of varibility in their datasets. 

#Differential expression analysis
##Create design matrix and contrasts
The design matrix and contrasts are how we will define our comparisons. You can make this as simple or as complicated as the experiment warrants, adding in temperature or time point factors as necessary. Again, there are many ways to do this, and I recommend that you read the edgeR and limma manuals. In R, you can use the `browseVignettes()` and the `vignette()` functions to access these guides.
The first matrix we create is the design matrix. In our experiment we are interested in comparing mutant genotype to our N2 control specimens. Earlier in this analysis we created the `group` variable and we can use this here to define our design matrix.

```{r}
##=====================##

design <- model.matrix(~0+group)
colnames(design) <- gsub("group", "", colnames(design))
design
```
The contrast matrix defines the comparisons that we would like to make. There are other ways to make this matrix and to perform the comparisons, but what I have done here is the most explicit method of doing this. This is helpful so that you understand exactly what comparisons are being made. Some of the other methods are more implicit and I don't see the benefit to them currently, apart from being slightly easier to type. Here, I am performing simple exp vs control comparisons; all genotypes are compared to N2.
```{r}
contrast.matrix <- makeContrasts(
    adrvsN2 = adr - N2,
    adr_rrf3vsN2 = adr_rrf3 - N2,
    adr_rrf3_rde1vsN2 = adr_rrf3_rde1 - N2,
    adr_rrf3_rde4vsN2 = adr_rrf3_rde4 - N2,
    rrf3vsN2 = rrf3 - N2,
    levels = colnames(design)
    )

contrast.matrix
```

##Remove heteroscedasity
The variance of RNA-seq data is not independant of the log-cpm mean. This is visualized by the graph on the left, showing higher variability in samples with lower counts. Variance indepednant of the mean is heteroscedacity, and the `voom()` function works to remove this heteroscedacity. This is critical for the next steps, the `lmFit()` function, which will fit a linear model gene-wise to the data. The `eBayes()` function then computes statistics for each gene, calculating the odds of differential expression in a gene-wise manner. (That is, no multiple-hypothesis testing; don't rely on these p values).
```{r}
par(mfrow=c(1,2))
v <- voom(x, design, plot = T)
vfit <- lmFit(v, design)
vfit <- contrasts.fit(vfit, contrasts = contrast.matrix)
efit <- eBayes(vfit)
plotSA(efit)
title(main = "Final model: Mean-variance trend")
```

##Examining differentially expressed genes
I will not be using a logFC criteria in this analysis. If one would like to do so, then use the `treat()` function instead of `eBayes()` function. The difference here is that `eBayes()` will compute the statistics that a given gene is differentially expressed. To do that it tests the null hypothesis, that the gene is expressed at the same level, i.e, that mut - wt = 0 or mut/wt = 1. The `treat()` function will compute the odds that a gene is expressed in mut at X-fold higher/lower than in wt (X being whatever you set it to). Thus, the null hypothesis is different. Note: if logFC in `treat()` is set to log2(1) (no fold change) then it is equivalent to `eBayes()`.

To identify genes that are signficantly up- or down-regulated, we use the `decideTests()` function, which will adjust the p-values generated by `ebayes()` and apply a significance level to all genes.
```{r}
dt <- decideTests(efit)
summary(dt)
```

##Visualize the above data with Venn diagrams
When analyzing datasets with more than two comparisons, it is possible to generate diagrams of 3 or more comparisons by adding additional columns of `dt`. 
```{r}
vennDiagram(dt[,1:2], circle.col = c("red", "blue"), include = "up", show.include = T)
```

#Session info
```{r}
sessionInfo()
```

