---
title: "Investigating IPR due to N. parisii and N. ausubeli infection"
output:
  html_document:
    toc: true
    toc_depth: 1
    df_print: paged
---
#Introduction  
We will be performing a differential expression analysis on data gathered by Emily during her time at the Broad Institute. To do so we will be using R and R "packages". Specifically, edgeR and limma, which will perform the heavy-lifting. We will also use Glimma later to make some interactive graphs. 
  
##Packages
A package is a bundle of code that generally extends the capabilities of R (or other language). In a generalized example, a user has a problem that takes several steps to complete and must be solved repeatedly. In response our user may write a package that contains a single function which performs all these steps. Now our user can install this package and solve the problem in a single step. 
  
##Reading this document  
This document is an R Notebook (an alternative to an R script) and was written in Rstudio using R markdown language. When reading this document, users will see a few different formats. First, there will be plain text like you're reading now. Second, are code blocks which will display the actual R code used to perform the calculations. They are generally gray or otherwise colored in and surrounded by a border. 

```{r}
#This is a code block
```

There may also be `some code` identified in the plain text. I will always name `variable` and `functions()` using this markup, and note the () after functions means it's a function.  

Third will be the output which will almost always follow a code block. Sometimes they may show `##` before each line:  
```{r}
#Code block followed by ##output
dim(cars)
```

While other times they may be an interactive piece of output:  
```{r}
#Click 'Next' to see all cars listed
head(cars, n=15)
```

or a graph:
```{r}
plot(cars$speed)
```

One important concept to understand is commenting. Commenting uses a `#` (in R and other languages, but not all) to tell the computer that the line following the `#` is meant for humans and should not be interpreted as code. See some of the previous code blocks to see this in action. 

#Setup
##Install packages
The first step is to install packages. In R there are a few ways of doing this. Most often the `install.packages()` will be used to fetch a package from the [CRAN repository](https://cran.r-project.org/). For our use, we use a different repository, [Bioconductor](https://bioconductor.org/), which has its own installer function, `biocLite()`. The first step here is to direct R to look at the bioconductor repository using `source()`, and then to use `biocLite()` to install the necessary packages. 

```{r eval=F}
source("http://bioconductor.org/biocLite.R")
biocLite()
biocLite("edgeR")
biocLite("limma")
biocLite("Glimma")
```
Note: this step only needs to be completed once. After a package has been installed on your computer, you do not need to reinstall it. Although, you may need to update a package if a new version is released. 

##Load packages (libraries)  
The second step after package installation is to load the libraries into your R session. Installing the packages places the required files onto your computer, but you need to tell R that you want to access those files. This is important because as time goes on you will install more and more packages, and this could cause issues if they were all simultaneously loaded. It is good practice to only load the libraries you need. Loading libraries is something that must be performed everytime your R session is closed.
```{r}
library(edgeR)
library(limma)
```

In this particular instance, it looks like loading edgeR automatically loads the limma package. This is not always the case, and I am showing both libraries in order to be explicit.

#Import data
This matrix file was generated at the Broad institue. It is the processed data, the result of mapping (via [bowtie2](http://bowtie-bio.sourceforge.net/bowtie2/index.shtml)) and counts quantified (using [RSEM](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-323)). There are a variety of ways to do this, and this is just one example. I use `dim()` to check the dimensions of this matrix (row, column), and `head()` to inspect the first 6 rows of the matrix.  

Note: In this case we have a single matrix file, but this is not always true. In many instances each sample will have an individual file with the raw count data.
```{r}
genes_rsem <- read.csv("nematocida.counts.matrix", stringsAsFactors = F, check.names = F)
dim(genes_rsem)
head(genes_rsem)
```

Note: the arguments in the `read.csv()` function were necessary for this analysis, which I discovered by trial and error, but that may not always be true.

##Set up counts matrix
We will use only a subset of the data in this case. We are only interested in the Control, ERTm1, and ERTm2 samples from L004. To do this, I make a new matrix consisting of the data columns we are interested in and the gene names are stored as rownames of this matrix. I then check the `dim()` to make sure I have all of the rows, and check `head()` to make sure I have the correct columns and that the rows are named properly.
```{r}
counts_matrix <- genes_rsem[,2:10]
rownames(counts_matrix) <- genes_rsem[,1]
colnames(counts_matrix)
dim(counts_matrix)
head(counts_matrix)
```

##Generate DGEList object
There are a number of ways to complete the previous two steps, and it's not critical how they are accopmlished. The end product of these import data steps is to have all the necessary data loaded into R in order to generate a DGEList object. A DGEList object is a class of data storage object that edgeR and limma functions can read and operate on.  
```{r}
x <- DGEList(counts=counts_matrix, genes = row.names(counts_matrix))
class(x)
x
```

NOTE: There is a second function, `readDGE()`, that will also generate our DGElist object. `readDGE()` may be used to concatenate several sample files into a single DGElist. Since our data was provided in a single matrix file we used the `DGEList()` function. 

#Organize data
Now that we have imported our data and created our DGEList object we move to the next step. Organizing the data is a crucial step for almost any analysis. We are going to revise the names, assign experimental groups (e.g., infected or uninfected, wt or mutant, etc.). Many of these steps are not set in stone and must be adapted for each new experiment.  

##Simplify names
This step is not necessary, but it is a quality of life improvement that will make things easier moving forward. I use the `substring()` and `gsub()` functions to remove characters I am not interested in and shorten the sample names.  
```{r}
samplenames <- substring(colnames(x),7,nchar(colnames(x)))
samplenames <- gsub("_L004","", samplenames)
colnames(x) <- samplenames
samplenames
```

##Group by treatments
All samples are of the same strain, same genotype, raised at the same temperature, were processed in the same RNA-seq lane. Therefore, we will not include these factors here. Other factors that could arise: date harvested, date sequenced, treated with drug, and many others. In general, one should be aware of "batch" effects, that is, some factor that contributes to variabilty other than our experimental factors. Those sources of variability could be any of the factors listed already, and any number of other items. Further down the workflow, we will see how to investigate some of these potential batch effects.  

For our analysis, we only have one variable, which is treatment with ERTmx or untreated. The following steps assign the treatment group to the samples in the `$group` slot.

```{r}
treat <- as.factor(rep(c("un", "ERTm1", "ERTm2"),3)) #Assigns exp. factors to groups
#Relevels factors to make control "un" the top/first factor
treat <- factor(treat, levels = c("un", "ERTm1", "ERTm2")) 
x$samples$group <- treat
x$samples
```

Looking at the above chart, we can see our 9 samples, and that each have been grouped into the proper treatment category. lib.size refers to the library size and is the sum of every value in the column. The norm.factors, are normalization factors which we have not calculated yet and are thus set to their default value of 1.  

Depending on the experiment, there may be more to do at this step (probably not less), but our analysis is relatively straight forward.

##Data pre-processing
Before we begin to analyze the data, there are certain steps to take to prepare the data.  

This first block is taking the counts-per-million (cpm) of the reads aswell as the logcpm. These values will be used further down to highlight some of the pre-processing we've performed.  
```{r}
cpm <- cpm(x)
lcpm <- cpm(x, log=T)
head(cpm)
```

##Remove lowly expressed genes
Here, we use the cpm values calculated previous to remove lowly expressed genes. We'll begin by looking at the size of our data object. In the case of a DGEList object, using the `dim()` function simply shows us how many rows/genes we have, and we'll perform the same test after we throw out lowely expressed genes. The functions we will use in this analysis are more robust when genes that are essentially zero are removed.  

1. First expression checks the number of genes that have zero counts in every sample.  

2. Second expression creates a vector of genes that have cpm >1 in at least three rows. We choose three because that is the number of replicates we have, however, this is an arbitrary number and the cutoff is flexible.  

3. The last expression keeps the genes that match the cutoff described above and refactors the library sizes now that we have dropped a number of genes along with their count data.


```{r}
dim(x)
#1. Check the number of genes that are zero in each sample
table(rowSums(x$counts==0)==12)
#2. Check and store the genes that meet our threshold
keep.exprs <- rowSums(cpm>1)>=3
#3. Keep the genes that meet our threshold, drop genes that do not
x <- x[keep.exprs,, keep.lib.sizes=F]
dim(x)
```

In this case, we've filtered out roughly, 9,000 genes. It should be noted that our cutoff is arbitrary. Here, we have set a threshold at genes which have a cpm of at least 1 in at least 3 samples. This threshold can be set at whatever you'd like, but it should be noted that having many genes with low counts tends to throw off the linear model and may produce false positives.

##Graphs showing filtering of low-count genes
These graphs show how the distribution of our data changes after our thresholding and pre-processing. On the left, we see the large spike of genes at very low counts. On the right, we see we have eliminated these low-count genes. 

```{r}
library(RColorBrewer)
nsamples <- ncol(x)
col <- brewer.pal(nsamples, 'Paired')
par(mfrow=c(1,2))
plot(density(lcpm[,1]), col=col[1], lwd=2, ylim=c(0,0.21), las=2, main="", xlab="")
title(main="Raw data", xlab="log-cpm")
abline(v=0, lty=3)
for (i in 2:nsamples) {
  den <- density(lcpm[,i])
  lines(den$x, den$y, col=col[i], lwd=2)
}
legend("topright", samplenames, text.col = col, bty="n", cex=0.6)
lcpm <- cpm(x, log=T)
plot(density(lcpm[,1]), col=col[1], lwd=2, ylim=c(0,0.21), las=2, main="", xlab="")
title(main="Filtered data", xlab="log-cpm")
abline(v=0, lty=3)
for (i in 2:nsamples) {
  den <- density(lcpm[,i])
  lines(den$x, den$y, col=col[i], lwd=2)
}
legend("topright", samplenames, text.col = col, bty="n", cex=0.6)
```

##Normalizing gene expression distributions
Our data is in raw scale prior to normalization. `calcNormFactors()` function here uses trimmed mean of M-values, "TMM", to normalize the samples based on library size. Picture it this way, we don't want to say that transcript x is more abundant in sample A versus sample B simply because sample A has more reads.
```{r}
x$samples$norm.factors
x <- calcNormFactors(x, method = "TMM")
x$samples$norm.factors
```

#Unsupervised clustering
##Looking for batch effects
Dimension 1 (x-axis in left plot) represents the largest contributor to the variance within the data. What this shows is that all ERTm1 and ERTm2 treated samples are quite similar to each other, while being quite different than untreated N2 samples. Differences in the replicates can be seen in the higher dimensions. In general, one would try to catch batch effects here. If samples were prepared on seperate dates, or ran on seperate lanes, a user could graph the samples here, labelled by their preparation dates, and see if samples cluster by date in a higher dimension. This would suggest a batch effect related to preparation date. Dealing with batch effects (other than ignoring them) is another issue and I won't address that here.
```{r}
lcpm <- cpm(x, log=T)
par(mfrow=c(1,2))
col.group <- treat
levels(col.group) <- brewer.pal(nlevels(col.group), "Set1")
col.group<-as.character(col.group)
plotMDS(lcpm, labels = treat, col=col.group)
title(main="Treatments")
plotMDS(lcpm, labels = treat, col=col.group, dim=c(3,4))
title(main="Treatments - dim 3,4")
```

#Differential expression analysis
##Create design matrix and contrasts
The design matrix and contrasts are how we will define our comparisons. Again, there are many ways to do this, and I recommend that you read the edgeR and limma manuals. In R, you can use the `browseVignettes()` and the `vignette()` functions to access these guides.
The first matrix we create is the design matrix. In this case we are only investigating ERTm1 or ERTm2 infections that we named earlier in this analysis the `treat` variable. You can make this as simple or as complicated as the experiment warrants, adding in temperature or time point factors as necessary. 
```{r}
design <- model.matrix(~0+treat)
colnames(design) <- gsub("treat", "", colnames(design)) #removes the word "treat" from columns
design
```
The contrast matrix defines the comparisons I want to make. There are other ways to make this matrix and to perform the comparisons, but what I have done here is the most explicit way of doing this. This is helpful so that you understand exactly what comparisons are being made. Some of the other methods are more implicit and I don't see the benefit to them currently, apart from being slightly easier to type.
```{r}
contrast.matrix <- makeContrasts(
  ERTm1vsN2 = ERTm1 - un,
  ERTm2vsN2 = ERTm2 - un,
  levels = colnames(design)
  )
contrast.matrix
```

##Remove heteroscedasity
The variance of RNA-seq data is not independant of the log-cpm mean. This is visualized by the graph on the left, showing higher variability in samples with lower counts. Variance indepednant of the mean is heteroscedacity, and the `voom()` function works to remove this heteroscedacity. This is critical for the next steps, the `lmFit()` function, which will fit a linear model gene-wise to the data. The `eBayes()` function then computes statistics for each gene, calculating the odds of differential expression in a gene-wise manner. (That is, no multiple-hypothesis testing; don't rely on these p values).
```{r}
par(mfrow=c(1,2))
v <- voom(x, design, plot = T)
vfit <- lmFit(v, design)
vfit <- contrasts.fit(vfit, contrasts = contrast.matrix)
efit <- eBayes(vfit)
plotSA(efit)
title(main = "Final model: Mean-variance trend")
```

##Examining differentially expressed genes
I will not be using a logFC criteria in this analysis. If one would like to do so, then use the `treat()` function instead of `eBayes()` function. The difference here is that `eBayes()` will compute the statistics that a given gene is differentially expressed. To do that it tests the null hypothesis, that the gene is expressed at the same level, i.e, that mut - wt = 0 or mut/wt = 1. The `treat()` function will compute the odds that a gene is expressed in mut at X-fold higher/lower than in wt (X being whatever you set it to). Thus, the null hypothesis is different. Note: if logFC in `treat()` is set to log2(1) (no fold change) then it is equivalent to `eBayes()`.

To identify genes that are signficantly up- or down-regulated, we use the `decideTests()` function, which will adjust the p-values generated by `ebayes()` and apply a significance level to all genes.
```{r}
dt <- decideTests(efit)
summary(dt)
```

##Visualize the above data with Venn diagrams
When analyzing datasets with more than two comparisons, it is possible to generate diagrams of 3 or more comparisons by adding additional columns of `dt`
```{r}
vennDiagram(dt[,1:2], circle.col = c("red", "blue"), include = "up", show.include = T)
```

##Use R to save common differentially expressed genes in a list
```{r}
de.common.up <- which(dt[,1] !=0 & dt[,2]!=0)
length(de.common.up)
common.genes <- efit$genes[de.common.up,1]
common.genes
#write.csv(common.genes, "2018_08_03_common_genes_up.csv")
```

##Use R to generate data objects containing all differentially expressed genes. 
The p.value cutoff can be adjusted to whatever you'd like.  
Note: that the cutoff refers to the adjusted p-value, which in our case is the FDR value. FDR is the output of a statistical test that incorporates multiple-hypothesis testing and should be the value that you refer to. It is also possible to include a logFC cutoff here. However, as stated above, there are reasons why that cutoff may not be accurate.
```{r}
p <- 0.05
ERTm1.vs.N2 <- topTable(efit, coef = 1, n=Inf, p.value = p)
dim(ERTm1.vs.N2)
ERTm1.vs.N2
ERTm2.vs.N2 <- topTable(efit, coef = 2, n=Inf, p.value = p)
dim(ERTm2.vs.N2)
ERTm2.vs.N2
```

#Heatmaps
The heatmaps show the relative expression of genes across all samples. Samples are averaged across the rows, so a dark red cell indicates a gene is more highly expressed in that sample than the average expression of the gene in all samples.

##Differentially expressed genes in ERTm1 treated animals (87 genes in total) ranked by FDR
```{r, fig.height=15, fig.width=10}
library(gplots)
i <- which(v$genes$genes %in% ERTm1.vs.N2[,1])
mycol <- colorpanel(1000,"blue","white","red")
heatmap.2(v$E[i,], scale = "row", labRow = v$genes$genes[i], labCol=treat, col=mycol, 
          trace="none", density.info = "none", margins = c(5,10), lhei = c(1,8), lwid = c(1,4),
          dendrogram = "column",cexRow = 1, cexCol = 1, keysize = 1)
```

##Top 100 differentially expressed genes in ERTm2 treated animals (ranked by FDR)
```{r fig.height=15, fig.width=10}
ERTm2.vs.N2.top <- ERTm2.vs.N2$genes[1:100]
i <- which(v$genes$genes %in% ERTm2.vs.N2.top)
heatmap.2(v$E[i,], scale = "row", labRow = v$genes$genes[i], labCol=treat, col=mycol, 
          trace="none", density.info = "none", margins = c(5,5), lhei = c(1,8), lwid = c(1,4),
          dendrogram = "column",cexRow = 1, cexCol = 1, keysize = 1)
```

##Mean difference (MD) plots
```{r}
plotMD(efit, column=1, status = dt[,1], main=colnames(efit)[1])
```
```{r}
plotMD(efit, column=2, status = dt[,2], main=colnames(efit)[2])
```

##Extracting gene lists
Here I extract more gene lists of interest.
```{r}
#Create vector containing commone UP gene names
de.common.up <- which(dt[,1] !=0 & dt[,2]!=0)
length(de.common.up)
common.genes <- efit$genes[de.common.up,1]
common.genes
#collect names and logFC values of common UP genes
ERTm1.vs.N2.common <- ERTm1.vs.N2[common.genes,2]
ERTm2.vs.N2.common <- ERTm2.vs.N2[common.genes,2]
de <- data.frame(ERTm1.vs.N2.common, ERTm2.vs.N2.common, row.names = ERTm1.vs.N2[common.genes,1])
de
```
```{r}
#Collect unique genes from both sets
#Unique for ERTm1
de.ertm1.unique <- which(dt[,1] !=0 & dt[,2]==0)
length(de.ertm1.unique)
ERTm1.vs.N2.unique <- efit$genes[de.ertm1.unique,1]
ERTm1.vs.N2.unique <- ERTm1.vs.N2[ERTm1.vs.N2.unique,1:2]
ERTm1.vs.N2.unique

#Unique UP for ERTm1 
de.ertm1.unique.up <- which(dt[,1] ==1 & dt[,2]==0)
length(de.ertm1.unique.up)
ERTm1.vs.N2.unique.UP <- efit$genes[de.ertm1.unique.up,1]
ERTm1.vs.N2.unique.UP <- ERTm1.vs.N2[ERTm1.vs.N2.unique.UP,1:2]
ERTm1.vs.N2.unique.UP

#unique for ERTm2
de.ertm2.unique <- which(dt[,1] ==0 & dt[,2]!=0)
length(de.ertm2.unique)
ERTm2.vs.N2.unique <- efit$genes[de.ertm2.unique,1]
ERTm2.vs.N2.unique <- ERTm2.vs.N2[ERTm2.vs.N2.unique,1:2]
ERTm2.vs.N2.unique

#unique UP for ERTm2
de.ertm2.unique.up <- de.ertm2.unique <- which(dt[,1] ==0 & dt[,2]==1)
length(de.ertm2.unique.up)
ERTm2.vs.N2.unique.UP <- efit$genes[de.ertm2.unique.up,1]
ERTm2.vs.N2.unique.UP <- ERTm2.vs.N2[ERTm2.vs.N2.unique.UP,1:2]
```

#Write to xlsx
This is one useful package to write and generate spreadsheets directly from R. Refer to the manual as this package is quite powerful. I have not found it possible to write and format the excel file precicely how I'd want, so ultimately you'll probably have to do some formatting within excel.
```{r eval=F}
library(openxlsx)

wb <- createWorkbook()
addWorksheet(wb, sheetName = "Summary")
addWorksheet(wb, sheetName = "Common UP")
addWorksheet(wb, sheetName = "ERTm1 UP unique")
addWorksheet(wb, sheetName = "ERTm2 UP unique")
addWorksheet(wb, sheetName = "ERTm1 all")
addWorksheet(wb, sheetName = "ERTm2 all")
addWorksheet(wb, sheetName = "IPR")

writeData(wb, "Summary", summary(dt, rowNames = F))
setColWidths(wb, sheet = 1, cols = 1:3, widths = "auto")
writeData(wb, "Common UP", de, rowNames = T)
setColWidths(wb, sheet = 1, cols = 1:3, widths = "auto")
writeData(wb, "ERTm1 UP unique", ERTm1.vs.N2.unique.UP)
setColWidths(wb, sheet = 2, cols = 1:2, widths = "auto")
writeData(wb, "ERTm2 UP unique", ERTm2.vs.N2.unique.UP)
setColWidths(wb, sheet = 3, cols = 1:2, widths = "auto")
writeData(wb, "ERTm1 all", ERTm1.vs.N2)
setColWidths(wb, sheet = 4, cols = 1:2, widths = "auto")
writeData(wb, "ERTm2 all", ERTm2.vs.N2)
setColWidths(wb, sheet = 5, cols = 1:2, widths = "auto")

saveWorkbook(wb, "2018_08_27_Nematocida_DEGenes.xlsx", overwrite = T)
```

#Glimma Plots
Glimma plots will generate an HTML file and some javascript files. Together, they can be opened in your internet browser window and are an interactive way to explore the dataset. 

##This Glimma plot is of ERTm1vsN2
```{r eval=F}
glMDPlot(efit, coef=1, status = dt[,1], main = colnames(efit)[1],
         side.main ="genes", counts = x$counts, groups = treat, launch = F,
         html = "2018_08_06_ERT1vsN2_MDplot")
```

##This Glimma plot is of ERTm2vsN2
```{r eval=F}
glMDPlot(efit, coef=2, status = dt[,2], main = colnames(efit)[2],
         side.main ="genes", counts = x$counts, groups = treat, launch = F,
         html = "2018_08_06_ERT2vsN2_MDplot")
```

#Session info
```{r}
sessionInfo()
```

